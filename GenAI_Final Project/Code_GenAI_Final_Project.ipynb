{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVv6TqPBUEjC"
      },
      "source": [
        "**No need to run everytime, because once data is extracted**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN2DpTPGTrKJ"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# # Specify the path to the zip file\n",
        "# zip_file_path = \"/content/drive/MyDrive/FashionVision_Dataset.zip\"  # Change this to the path of your zip file\n",
        "\n",
        "# # Specify the directory to unzip the contents\n",
        "# extract_dir = \"/content/drive/MyDrive\"  # Change this to the directory where you want to extract the files\n",
        "\n",
        "# # Create the directory if it doesn't exist\n",
        "# os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# # Unzip the file\n",
        "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_dir)\n",
        "\n",
        "# # Check the contents of the extracted directory\n",
        "# extracted_files = os.listdir(extract_dir)\n",
        "# print(\"Files extracted successfully:\", extracted_files)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation and Augumentation done**"
      ],
      "metadata": {
        "id": "MBBt5RARdAGO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MPYPUM9WR4C7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_dir = os.path.join(root_dir, 'images')\n",
        "        self.label_dir = os.path.join(root_dir, 'labels')\n",
        "        self.image_filenames = os.listdir(self.image_dir)\n",
        "        self.transform = transform\n",
        "        self.labels = self._read_labels()\n",
        "\n",
        "    def _read_labels(self):\n",
        "        labels = {}\n",
        "        for filename in os.listdir(self.label_dir):\n",
        "            if filename.endswith(\".txt\"):\n",
        "                with open(os.path.join(self.label_dir, filename), 'r') as file:\n",
        "                    label = int(file.readline().strip().split()[0])  # assuming label is the first integer in the text file\n",
        "                    labels[filename[:-4]] = label  # removing '.txt' extension from filename\n",
        "        return labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_filenames[idx]\n",
        "        image = cv2.imread(os.path.join(self.image_dir, img_name))\n",
        "        label = self.labels[img_name[:-4]]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Example usage:\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Convert numpy array to PIL image\n",
        "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
        "    transforms.ToTensor(),  # Convert PIL image to tensor\n",
        "    # Add any additional transformations here if needed\n",
        "])\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/FashionVision_Dataset\"\n",
        "dataset = CustomDataset(root_dir, transform=transform)\n",
        "\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Now you can iterate over the dataloader to get batches of resized images and labels\n",
        "for batch_idx, (images, labels) in enumerate(dataloader):\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "    print(\"Images shape:\", images.shape)  # Shape of the batch of images (should be [batch_size, channels, height, width])\n",
        "    print(\"Labels:\", labels)  # Labels corresponding to the batch of images\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])"
      ],
      "metadata": {
        "id": "_ns641UR0ucR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of training samples:\", len(train_dataset))\n",
        "print(\"Number of validation samples:\", len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBJ6RjAz1tAb",
        "outputId": "b6a4e3cd-3047-4e81-df74-0fee0dcc7459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 730\n",
            "Number of validation samples: 183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of unique classes in the dataset\n",
        "num_classes = len(torch.unique(torch.tensor(list(dataset.labels.values()))))\n",
        "print(\"Number of classes in the dataset:\", num_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brmDFWGv0aMo",
        "outputId": "fe2f21e4-fb7c-452d-e1ea-7d36f185c328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes in the dataset: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Define a function to display sample images\n",
        "# def show_images(images, labels, class_names, num_images=5):\n",
        "#     fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
        "#     for i in range(num_images):\n",
        "#         ax = axes[i]\n",
        "#         ax.imshow(np.transpose(images[i], (1, 2, 0)))\n",
        "#         ax.set_title(class_names[labels[i]])\n",
        "#         ax.axis(\"off\")\n",
        "#     plt.show()\n",
        "\n",
        "# # Get a batch of sample images and labels\n",
        "# sample_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
        "# sample_batch = next(iter(sample_loader))\n",
        "# sample_images, sample_labels = sample_batch\n",
        "\n",
        "# # Display the sample images\n",
        "# show_images(sample_images, sample_labels, dataset.class_names)\n"
      ],
      "metadata": {
        "id": "4WBk-NU60qWf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TZeprjVUUlZo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def show_images(images, labels, num_images=4):\n",
        "    \"\"\"\n",
        "    Display images from a batch along with their corresponding labels.\n",
        "\n",
        "    Args:\n",
        "        images (torch.Tensor): Batch of images with shape (batch_size, channels, height, width).\n",
        "        labels (torch.Tensor): Batch of labels corresponding to the images.\n",
        "        num_images (int): Number of images to display. Default is 4.\n",
        "    \"\"\"\n",
        "    # Ensure num_images is not greater than the batch size\n",
        "    num_images = min(num_images, len(images))\n",
        "\n",
        "    # Create a grid of subplots\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
        "    image_count=0\n",
        "    for i in range(num_images):\n",
        "        image_count+=1\n",
        "        # Convert the tensor image to numpy array and transpose to (height, width, channels)\n",
        "        image_np = np.transpose(images[i].numpy(), (1, 2, 0))\n",
        "\n",
        "        # Display the image\n",
        "        axes[i].imshow(image_np)\n",
        "        axes[i].set_title(f\"Label: {labels[i]}\")\n",
        "        axes[i].axis('off')\n",
        "    print(\"Number of images:\", image_count)\n",
        "    #plt.show()\n",
        "\n",
        "# Example usage:\n",
        "batch_count=0\n",
        "for batch_idx, (images, labels) in enumerate(dataloader):\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "    #batch_count+=1\n",
        "    show_images(images, labels)\n",
        "print(\"Batches: \", batch_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXrBlwbBaVrD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Implementation**"
      ],
      "metadata": {
        "id": "jkNCQpn2EgNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domain_x_path = \"/content/sample_data/domain_x_dataset\"  # Fabric patches\n",
        "domain_y_path = \"/content/sample_data/domain_y_dataset\"  # Full dress images\n",
        "\n",
        "domain_x_dataset = datasets.ImageFolder(domain_x_path, transform=transform)\n",
        "domain_y_dataset = datasets.ImageFolder(domain_y_path, transform=transform)\n",
        "\n",
        "domain_x_loader = DataLoader(domain_x_dataset, batch_size=1, shuffle=True)\n",
        "domain_y_loader = DataLoader(domain_y_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            # Add your layers here\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            # Add your layers here\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "G = Generator()\n",
        "F = Generator()\n",
        "D_X = Discriminator()\n",
        "D_Y = Discriminator()\n",
        "\n",
        "G_optimizer = optim.Adam(G.parameters(), lr=0.0002)\n",
        "F_optimizer = optim.Adam(F.parameters(), lr=0.0002)\n",
        "D_X_optimizer = optim.Adam(D_X.parameters(), lr=0.0002)\n",
        "D_Y_optimizer = optim.Adam(D_Y.parameters(), lr=0.0002)\n",
        "\n",
        "adversarial_loss = nn.MSELoss()\n",
        "cycle_loss = nn.L1Loss()\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for (x_batch, _), (y_batch, _) in zip(domain_x_loader, domain_y_loader):\n",
        "        # Train Discriminator D_X\n",
        "        D_X_optimizer.zero_grad()\n",
        "        real_x = x_batch\n",
        "        fake_x = F(y_batch)\n",
        "        real_x_output = D_X(real_x)\n",
        "        fake_x_output = D_X(fake_x.detach())\n",
        "        D_X_loss = adversarial_loss(real_x_output, torch.ones_like(real_x_output)) + adversarial_loss(fake_x_output, torch.zeros_like(fake_x_output))\n",
        "        D_X_loss.backward()\n",
        "        D_X_optimizer.step()\n",
        "\n",
        "        # Train Discriminator D_Y\n",
        "        D_Y_optimizer.zero_grad()\n",
        "        real_y = y_batch\n",
        "        fake_y = G(x_batch)\n",
        "        real_y_output = D_Y(real_y)\n",
        "        fake_y_output = D_Y(fake_y.detach())\n",
        "        D_Y_loss = adversarial_loss(real_y_output, torch.ones_like(real_y_output)) + adversarial_loss(fake_y_output, torch.zeros_like(fake_y_output))\n",
        "        D_Y_loss.backward()\n",
        "        D_Y_optimizer.step()\n",
        "\n",
        "        # Train Generators G and F\n",
        "        G_optimizer.zero_grad()\n",
        "        F_optimizer.zero_grad()\n",
        "\n",
        "        fake_y = G(x_batch)\n",
        "        fake_y_output = D_Y(fake_y)\n",
        "        G_adversarial_loss = adversarial_loss(fake_y_output, torch.ones_like(fake_y_output))\n",
        "        reconstructed_x = F(fake_y)\n",
        "        cycle_x_loss = cycle_loss(reconstructed_x, x_batch)\n",
        "        G_loss = G_adversarial_loss + cycle_x_loss\n",
        "\n",
        "        G_loss.backward()\n",
        "        G_optimizer.step()\n",
        "\n",
        "        fake_x = F(y_batch)\n",
        "        fake_x_output = D_X(fake_x)\n",
        "        F_adversarial_loss = adversarial_loss(fake_x_output, torch.ones_like(fake_x_output))\n",
        "        reconstructed_y = G(fake_x)\n",
        "        cycle_y_loss = cycle_loss(reconstructed_y, y_batch)\n",
        "        F_loss = F_adversarial_loss + cycle_y_loss\n",
        "\n",
        "        F_loss.backward()\n",
        "        F_optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], D_X Loss: {D_X_loss.item()}, D_Y Loss: {D_Y_loss.item()}, G Loss: {G_loss.item()}, F Loss: {F_loss.item()}\")\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        torch.save(G.state_dict(), f\"G_epoch{epoch}.pth\")\n",
        "        torch.save(F.state_dict(), f\"F_epoch{epoch}.pth\")\n",
        "        torch.save(D_X.state_dict(), f\"D_X_epoch{epoch}.pth\")\n",
        "        torch.save(D_Y.state_dict(), f\"D_Y_epoch{epoch}.pth\")\n"
      ],
      "metadata": {
        "id": "PDnqCVr5EfOE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional GANs"
      ],
      "metadata": {
        "id": "9tdCExMAFjqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domain_x_path = \"/content/sample_data/domain_x_dataset\"\n",
        "domain_y_path = \"/content/sample_data/domain_y_dataset\"\n",
        "\n",
        "\n",
        "domain_x_dataset = datasets.ImageFolder(domain_x_path, transform=transform)\n",
        "domain_y_dataset = datasets.ImageFolder(domain_y_path, transform=transform)\n",
        "\n",
        "domain_x_loader = DataLoader(domain_x_dataset, batch_size=1, shuffle=True)\n",
        "domain_y_loader = DataLoader(domain_y_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_channels=3, output_channels=3, condition_dim=10):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(input_channels + condition_dim, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, output_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, condition):\n",
        "        input_data = torch.cat((x, condition), dim=1)\n",
        "        return self.model(input_data)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_channels=3, condition_dim=10):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(input_channels + condition_dim, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 64 * 64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, condition):\n",
        "        input_data = torch.cat((x, condition), dim=1)\n",
        "        return self.model(input_data)\n",
        "\n",
        "G = Generator()\n",
        "D = Discriminator()\n",
        "\n",
        "generator = Generator(input_channels=3, output_channels=3, condition_dim=10)\n",
        "discriminator = Discriminator(input_channels=3, condition_dim=10)\n",
        "\n",
        "fake_image = generator(input_image, condition)\n",
        "discriminator_output = discriminator(fake_image, condition)\n",
        "\n",
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for (x_batch, _), (y_batch, _) in zip(domain_x_loader, domain_y_loader):\n",
        "        condition = generate_condition()  # Customize condition generation\n",
        "\n",
        "        D_optimizer.zero_grad()\n",
        "        real_y = y_batch\n",
        "        fake_y = G(x_batch, condition)\n",
        "        real_y_output = D(real_y, condition)\n",
        "        fake_y_output = D(fake_y.detach(), condition)\n",
        "        D_loss = adversarial_loss(real_y_output, torch.ones_like(real_y_output)) + adversarial_loss(fake_y_output, torch.zeros_like(fake_y_output))\n",
        "        D_loss.backward()\n",
        "        D_optimizer.step()\n",
        "\n",
        "        G_optimizer.zero_grad()\n",
        "        fake_y = G(x_batch, condition)\n",
        "        fake_y_output = D(fake_y, condition)\n",
        "        G_loss = adversarial_loss(fake_y_output, torch.ones_like(fake_y_output))\n",
        "        G_loss.backward()\n",
        "        G_optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], D Loss: {D_loss.item()}, G Loss: {G_loss.item()}\")\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        torch.save(G.state_dict(), f\"G_epoch{epoch}.pth\")\n",
        "        torch.save(D.state_dict(), f\"D_epoch{epoch}.pth\")\n"
      ],
      "metadata": {
        "id": "bCR1ejMaE_q1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pNPPlrKrGXpw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}